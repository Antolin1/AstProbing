{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Download the results of the experiments"
   ],
   "metadata": {
    "id": "lqf8wojksFUR"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We download the results that we have used in the paper."
   ],
   "metadata": {
    "id": "TsA62_qmhwr3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install --upgrade --no-cache-dir gdown\n",
    "!gdown 17grO477tqFbYT3ISJRn18vqoVD0dbjcD\n",
    "!tar -xf AstProbing.tar.gz\n",
    "!apt-get install tree"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jWZjufNRvXz0",
    "outputId": "4a653bf0-b0f9-466f-9a9a-673c554dca2e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `dataset` folder contains each folder per language. Each folder contains the splits (train, test, and valid), the full dataset, and a serialized vocabulary with the constituency and unary labels."
   ],
   "metadata": {
    "id": "DGhWKXSiixli"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!tree dataset | head -n 7"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HfkItFq0inWR",
    "outputId": "fd2d741c-3495-43cd-c856-3012ce2d8b4f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The runs `folders` contains the output of each one of the probing runs. In particular, the format of the folders is `plm_lang_layer_dimension`. Each run folder contains a log, a serialized file with the test results, and the pytorch model. Finally, if the run folder corresponds to a multilingual setting, it starts with `multilingual`.\n",
    "\n"
   ],
   "metadata": {
    "id": "sR-Ueo9Mj-JJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!tree runs | head -n 12"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QcQO4kN6kANw",
    "outputId": "f31259d9-1084-4154-ea76-186ad07c35c6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, the folder finetining_results contains the results for each PLM in the code related tasks."
   ],
   "metadata": {
    "id": "KWFFksWroOWw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!cat finetuning_results/code_search_go.json"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zVH-65o8od2j",
    "outputId": "25f2dd38-6789-4da4-b0bd-6158e9b21975"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reading and preprocessing the data"
   ],
   "metadata": {
    "id": "SpbyLMEbsLhK"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We load the results of the monolingual and multilingual probe into two pandas dataframe (`mono` and `mulit`)."
   ],
   "metadata": {
    "id": "bmnXiZZJlfCb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ELEGANT_NAMES = {'codebert': 'CodeBERT',\n",
    "                 'codebert-baseline': 'CodeBERTrand',\n",
    "                 'codeberta': 'CodeBERTa',\n",
    "                 'codet5': 'CodeT5',\n",
    "                 'graphcodebert': 'GraphCodeBERT',\n",
    "                 'roberta': 'RoBERTa',\n",
    "                 'distilbert': 'DistilBERT',\n",
    "                 'bert': 'BERT',\n",
    "                 'distilroberta': 'DistilRoBERTa'\n",
    "                 }\n",
    "LANGUAGES = (\n",
    "    'python',\n",
    "    'java',\n",
    "    'ruby',\n",
    "    'javascript',\n",
    "    'go',\n",
    "    'c',\n",
    "    'csharp',\n",
    "    'php'\n",
    ")"
   ],
   "metadata": {
    "id": "XExk6Vehv7wq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def read_results_mono(run_dir='runs'):\n",
    "    data = {'model': [], 'lang': [], 'layer': [], 'rank': [],\n",
    "            'precision': [], 'recall': [], 'f1': []}\n",
    "    for file in glob.glob(run_dir + \"/*/metrics.log\"):\n",
    "        parent = os.path.dirname(file).split('/')[-1]\n",
    "        if 'multilingual' in parent:\n",
    "            continue\n",
    "        model, lang, layer, rank = parent.split('_')\n",
    "        with open(file, 'rb') as f:\n",
    "            results = pickle.load(f)\n",
    "        data['model'].append(model)\n",
    "        data['lang'].append(lang)\n",
    "        data['layer'].append(int(layer))\n",
    "        data['rank'].append(int(rank))\n",
    "        data['precision'].append(results['test_precision'])\n",
    "        data['recall'].append(results['test_recall'])\n",
    "        data['f1'].append(results['test_f1'])\n",
    "    df = pd.DataFrame(data)\n",
    "    df_renamed = df.replace(ELEGANT_NAMES)\n",
    "    return df_renamed\n",
    "\n",
    "def read_results_multi(run_dir='runs'):\n",
    "    data = {'model': [], 'lang': [], 'recall': [], 'f1': [], 'precision': []}\n",
    "    for file in glob.glob(run_dir + \"/*/metrics.log\"):\n",
    "        parent = os.path.dirname(file).split('/')[-1]\n",
    "        if 'multilingual' not in parent:\n",
    "            continue\n",
    "        _, model = parent.split('_')\n",
    "        with open(file, 'rb') as f:\n",
    "            results = pickle.load(f)\n",
    "        for lang in LANGUAGES:\n",
    "            data['model'].append(model)\n",
    "            data['lang'].append(lang)\n",
    "            data['precision'].append(results[f'test_precision_{lang}'])\n",
    "            data['recall'].append(results[f'test_recall_{lang}'])\n",
    "            data['f1'].append(results[f'test_f1_{lang}'])\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ],
   "metadata": {
    "id": "yrvz7mbHv_qg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "mono = read_results_mono(run_dir='runs')\n",
    "multi = read_results_multi(run_dir='runs')"
   ],
   "metadata": {
    "id": "XZHhSg4awNIb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataframe `mono` contains the results of the monolingual probe for each model, layer, and programming language."
   ],
   "metadata": {
    "id": "X4N5ZSgHl8BH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "mono.head(5)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "fhGlyTHrl6qC",
    "outputId": "995354ae-46b4-42c7-ee01-9cb94b615574"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataframe `multi` contains the results of the multilingual probe for each model and programing language. The mutlingual probe was run over the representative layer of the PLM (i.e., the layer that achieved the best average F1 score). "
   ],
   "metadata": {
    "id": "PsPEUUe9mMv7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "multi.head(5)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "N4PPF_QCmJot",
    "outputId": "b8e253aa-dff1-41cc-f6fe-5f095e82cf51"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Computing the syntactic layer for each model"
   ],
   "metadata": {
    "id": "3D-sxfJfshot"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now compute the layer that achieve the best average F1 score for each model (`best_layer_mono` dataframe)."
   ],
   "metadata": {
    "id": "VM3nMK38m28K"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# This selects the representative layer\n",
    "def best_layer_for_each_model(results):\n",
    "    group_by_model = results.groupby(['model', 'layer'])['f1'].mean().reset_index()\n",
    "    best_layer_per_model = (\n",
    "        group_by_model\n",
    "        .groupby(['model'])\n",
    "        .apply(lambda group: group.loc[group['f1'] == group['f1'].max()])\n",
    "        .reset_index(level=-1, drop=True)\n",
    "    )\n",
    "    return best_layer_per_model"
   ],
   "metadata": {
    "id": "diiQteDqskOz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "best_layer_mono = best_layer_for_each_model(mono)\n",
    "best_layer_mono.head(10)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "PzhVs8KcswAO",
    "outputId": "1ea9d267-825d-413b-a82e-4454a89f7b58"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plots layer vs F1"
   ],
   "metadata": {
    "id": "Ga4Kjmn7tWZ4"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Given a programming language, we plot the layer vs F1 score for each model."
   ],
   "metadata": {
    "id": "sU01FoV8ngDI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from plotnine import *\n",
    "\n",
    "lang = 'python'\n",
    "assert lang in LANGUAGES\n",
    "\n",
    "layer_vs_f1_lang = (\n",
    "  ggplot(mono[(mono['lang'] == lang)])\n",
    "  + aes(x=\"layer\", y=\"f1\", color='model')\n",
    "  + geom_line()\n",
    "  + scale_x_continuous(breaks=range(0, 13, 1))\n",
    "  + labs(x=\"Layer\", y=\"F1\", color=\"Model\")\n",
    "  + theme(text=element_text(size=16))\n",
    "  + theme_light()\n",
    ")\n",
    "layer_vs_f1_lang"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "id": "NgmvwPR-uBpn",
    "outputId": "6280ccbc-93e9-4ea6-875d-fd6380f6155e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Given a PLM, we plot the layer vs F1 for each language."
   ],
   "metadata": {
    "id": "GOOqvdoMnvY4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = 'GraphCodeBERT'\n",
    "assert model in ELEGANT_NAMES.values()\n",
    "\n",
    "layer_vs_f1_model = (\n",
    "  ggplot(mono[(mono['model'] == model)])\n",
    "  + aes(x=\"layer\", y=\"f1\", color='lang')\n",
    "  + geom_line()\n",
    "  + scale_x_continuous(breaks=range(0, 13, 1))\n",
    "  + labs(x=\"Layer\", y=\"F1\", color=\"Lang\")\n",
    "  + theme(text=element_text(size=16))\n",
    "  + theme_light()\n",
    ")\n",
    "layer_vs_f1_model"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "id": "aB5XYzTSvBkq",
    "outputId": "55f52c78-71e8-41d2-9f8c-108a265c4b5b"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Correlations"
   ],
   "metadata": {
    "id": "Dg2fEIu_vXEM"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now compute the (Spearman) correlations of the multilingual AST-Probe and the performances on downstream tasks."
   ],
   "metadata": {
    "id": "0uJhQu-Zpat2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "def compute_correlation(rq_dataframe, results_finetuning, task_name, verbose=True):\n",
    "    input_lang = results_finetuning[\"input_lang\"]\n",
    "    results_finetuning_pd = pd.DataFrame.from_dict({\"model\": results_finetuning[\"model\"],\n",
    "                                                    \"performance\": results_finetuning[\"performance\"]})\n",
    "    if len(input_lang) == 1:\n",
    "        rq2_dataframe_lang = rq_dataframe[rq_dataframe[\"lang\"] == input_lang[0]]\n",
    "    else:\n",
    "        rq2_dataframe_lang = rq_dataframe[rq_dataframe[\"lang\"].isin(input_lang)] \\\n",
    "            .groupby(['model'])['f1'].mean().reset_index()\n",
    "\n",
    "    df_cd = pd.merge(results_finetuning_pd, rq2_dataframe_lang, how='inner', on='model')\n",
    "    f1s = list(df_cd[\"f1\"])\n",
    "    performances = list(df_cd[\"performance\"])\n",
    "    models = list(df_cd[\"model\"])\n",
    "    if verbose:\n",
    "      print(f'Task: {task_name}')\n",
    "      print(f'Models: {models}')\n",
    "      print(f'F1s: {f1s}')\n",
    "      print(f'Performances: {performances}')\n",
    "      print(f'Correlation: {stats.spearmanr(f1s, performances)}')\n",
    "    return stats.spearmanr(f1s, performances).correlation"
   ],
   "metadata": {
    "id": "sPJuk7bmxs6C"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "finetuning_results_path = 'finetuning_results/defect_prediction.json'\n",
    "with open(finetuning_results_path) as json_file:\n",
    "  results_finetuning = json.load(json_file)\n",
    "_ = compute_correlation(multi, results_finetuning, 'defect prediction')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y7O_zx8-x8J8",
    "outputId": "24c9d287-cb37-47d9-8747-da07bb3f9a7e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "For each task and input programming language, the correlations are the following:"
   ],
   "metadata": {
    "id": "snETuGUQp7Wl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "code2nl = []\n",
    "code_search = []\n",
    "code2code = []\n",
    "for file in glob.glob('finetuning_results/*.json'):\n",
    "  with open(file) as json_file:\n",
    "    results_finetuning = json.load(json_file)\n",
    "  _, tail = os.path.split(file)\n",
    "  task, _ = tail.split('.')\n",
    "  corr = compute_correlation(multi, results_finetuning, task, False)\n",
    "  if 'code2nl' in task:\n",
    "    code2nl.append(corr)\n",
    "  elif 'code_search' in task:\n",
    "    code_search.append(corr)\n",
    "  elif task == 'java_to_csharp' or task == 'csharp_to_java':\n",
    "    code2code.append(corr)\n",
    "  print(f'Correlation for {task}: {corr}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "koUlAD_j3mTH",
    "outputId": "8912e6eb-3b52-47ff-8269-531030d928f9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The average correlations are:"
   ],
   "metadata": {
    "id": "FKj5e6AHqCxJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "np.mean(code2nl)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8_7WAGVF7dHu",
    "outputId": "73a5c1a4-c576-488b-b91d-56ffadd40648"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "np.mean(code_search)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1r3EqyjX7s8l",
    "outputId": "1cd5ab1d-6e84-4123-e74c-edb187172a04"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "np.mean(code2code)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YeTIvvsR8PSt",
    "outputId": "dd2d17d1-e6ac-4cfb-ba40-0d168c0d21b5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The scatter plots (Performance vs F1) can be drawn using the following snippet. You can modify `finetuning_results_path`.."
   ],
   "metadata": {
    "id": "AsqSYBrgrBR2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from scipy.stats import rankdata\n",
    "\n",
    "def scatter_plot_correlations(rq_dataframe, results_finetuning, rank=True):\n",
    "  input_lang = results_finetuning[\"input_lang\"]\n",
    "  results_finetuning_pd = pd.DataFrame.from_dict({\"model\": results_finetuning[\"model\"],\n",
    "                                                    \"performance\": results_finetuning[\"performance\"]})\n",
    "  if len(input_lang) == 1:\n",
    "    rq2_dataframe_lang = rq_dataframe[rq_dataframe[\"lang\"] == input_lang[0]]\n",
    "  else:\n",
    "    rq2_dataframe_lang = rq_dataframe[rq_dataframe[\"lang\"].isin(input_lang)] \\\n",
    "            .groupby(['model'])['f1'].mean().reset_index()\n",
    "\n",
    "  df_cd = pd.merge(results_finetuning_pd, rq2_dataframe_lang, how='inner', on='model')\n",
    "  f1s = np.array(df_cd[\"f1\"])\n",
    "  if rank:\n",
    "    f1s = rankdata(f1s, method='ordinal')\n",
    "  performances = np.array(df_cd[\"performance\"])\n",
    "  if rank:\n",
    "    performances = rankdata(performances, method='ordinal')\n",
    "  p_dict = {\"F1\": f1s, \"Performance\": performances}\n",
    "  df = pd.DataFrame(p_dict)\n",
    "  scatter_plot = (\n",
    "    ggplot(df)\n",
    "    + aes(x=\"Performance\", y=\"F1\")\n",
    "    + geom_point()\n",
    "    + geom_smooth(method='lm', se=True)\n",
    "    + theme(text=element_text(size=16))\n",
    "    + theme_light()\n",
    "    + labs(title=\"\", x=\"Rank Performance\" if rank else \"Performance\", \n",
    "           y=\"Rank F1\" if rank else \"F1\")\n",
    "  )\n",
    "  return scatter_plot\n",
    "  \n",
    "finetuning_results_path = 'finetuning_results/code_search_go.json'\n",
    "with open(finetuning_results_path) as json_file:\n",
    "  results_finetuning = json.load(json_file)\n",
    "scatter_plot_correlations(multi, results_finetuning, rank=False)"
   ],
   "metadata": {
    "id": "4JlfVZG6Y4SD",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "outputId": "591ab734-679b-4254-c3c8-70eb59e7b506"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualization"
   ],
   "metadata": {
    "id": "QnBdruigzk9Q"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we present the visualization techniques presented in the paper."
   ],
   "metadata": {
    "id": "CcyXmlBhsUmW"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Angle between subspaces"
   ],
   "metadata": {
    "id": "Sqsy1EUziL24"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To visualize angle between subspaces, the following code load the projections and compute the angles between each pair of programming languages."
   ],
   "metadata": {
    "id": "psC2gcgVrgiZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from prettytable import PrettyTable\n",
    "from scipy.linalg import subspace_angles\n",
    "\n",
    "def load_vectors(run_folder):\n",
    "    loaded_model = torch.load(os.path.join(run_folder, f'pytorch_model.bin'),\n",
    "                              map_location=torch.device('cpu'))\n",
    "    vectors_c = loaded_model['vectors_c'].cpu().detach().numpy().T\n",
    "    vectors_u = loaded_model['vectors_u'].cpu().detach().numpy().T\n",
    "    proj = loaded_model['proj'].cpu().detach().numpy()\n",
    "    return vectors_c, vectors_u, proj\n",
    "  \n",
    "def compute_angle_model(best_layer_per_model, model):\n",
    "    layer = best_layer_per_model[best_layer_per_model['model'] == ELEGANT_NAMES[model]].layer.values[0]\n",
    "    subspaces = {}\n",
    "    for lang in LANGUAGES:\n",
    "        name_folder = '_'.join([model, lang, str(layer), '128'])\n",
    "        run_folder = os.path.join('runs', name_folder)\n",
    "        _, _, proj = load_vectors(run_folder)\n",
    "        subspaces[lang] = proj\n",
    "\n",
    "    table_sim_ang = PrettyTable()\n",
    "    table_sim_ang.field_names = [\"----\"] + list(LANGUAGES)\n",
    "\n",
    "    data = {'lang1': [], 'lang2': [], 'angle': [], 'text': [], 'model': []}\n",
    "    for i, x in enumerate(LANGUAGES):\n",
    "        row_ang = [x]\n",
    "        for j, y in enumerate(LANGUAGES):\n",
    "            subspace_sim_ang = np.rad2deg(np.mean(subspace_angles(subspaces[x], subspaces[y])))\n",
    "            row_ang.append(round(subspace_sim_ang, 2))\n",
    "            if x != y and j < i:\n",
    "                data['lang1'].append(x)\n",
    "                data['lang2'].append(y)\n",
    "                data['angle'].append(subspace_sim_ang)\n",
    "                data['text'].append(str(round(subspace_sim_ang, 2)))\n",
    "                data['model'].append(model)\n",
    "        table_sim_ang.add_row(row_ang)\n",
    "\n",
    "    df = pd.DataFrame.from_dict(data)\n",
    "    return df, table_sim_ang"
   ],
   "metadata": {
    "id": "sOMktBglfoJ5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = 'codebert'\n",
    "assert model in ELEGANT_NAMES\n",
    "\n",
    "df, pt = compute_angle_model(best_layer_mono, model)\n",
    "# print(pt)\n",
    "angles_p9 = (\n",
    "            ggplot(mapping=aes(\"lang1\", \"lang2\", fill=\"angle\"),\n",
    "                   data=df)\n",
    "            + geom_tile() + geom_label(aes(label=\"text\"), fill=\"white\", size=10)\n",
    "            + scale_fill_distiller()\n",
    "            + theme_minimal()\n",
    "            + scale_x_discrete(limits=LANGUAGES)\n",
    "            + scale_y_discrete(limits=LANGUAGES)\n",
    "            + labs(title=\"\", x=\"\", y=\"\", fill=\"angle\\n\")\n",
    "            + theme(axis_text_x=element_text(rotation=45, hjust=1, size=12),\n",
    "                    axis_text_y=element_text(size=12),\n",
    "                    legend_title=element_text(size=12),\n",
    "                    legend_title_align='center')\n",
    "    )\n",
    "angles_p9"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "id": "Xsef8MCohs1g",
    "outputId": "734785da-e681-4243-e5f8-f83124464438"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualization of constituency labels"
   ],
   "metadata": {
    "id": "Is3DlFo4sbkb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def load_labels(run_folder):\n",
    "    labels_file_path_c = os.path.join(run_folder, 'global_labels_c.pkl')\n",
    "    labels_file_path_u = os.path.join(run_folder, 'global_labels_u.pkl')\n",
    "    with open(labels_file_path_c, 'rb') as f:\n",
    "        labels_to_ids_c = pickle.load(f)\n",
    "    with open(labels_file_path_u, 'rb') as f:\n",
    "        labels_to_ids_u = pickle.load(f)\n",
    "    ids_to_labels_c = {y: x for x, y in labels_to_ids_c.items()}\n",
    "    ids_to_labels_u = {y: x for x, y in labels_to_ids_u.items()}\n",
    "    return labels_to_ids_c, ids_to_labels_c, labels_to_ids_u, ids_to_labels_u\n",
    "\n",
    "def run_tsne(vectors, ids_to_labels, perplexity=30, type_labels='constituency', seed=123):\n",
    "    v_2d = TSNE(n_components=2, learning_rate='auto', perplexity=perplexity,\n",
    "                init='random', random_state=seed).fit_transform(vectors)\n",
    "    df = pd.DataFrame(v_2d, columns=['tsne1', 'tsne2'])\n",
    "    langs = []\n",
    "    const = []\n",
    "    for ix, _ in enumerate(ids_to_labels):\n",
    "        label = ids_to_labels[ix]\n",
    "        l = label.split('--')[1]\n",
    "        langs.append(l)\n",
    "        const.append(label.split('--')[0])\n",
    "    df['language'] = langs\n",
    "    df['constituency'] = const\n",
    "    scatter_tsne = (\n",
    "            ggplot(df, aes(x='tsne1', y='tsne2', color='language')) + geom_point()\n",
    "            + labs(title=\"\", x=\"\", y=\"\", color=\"Languages\")\n",
    "    )\n",
    "    return scatter_tsne"
   ],
   "metadata": {
    "id": "jThZmnbotKNi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = 'CodeBERT'\n",
    "assert model in ELEGANT_NAMES.values()\n",
    "\n",
    "run_folder = f'runs/multilingual_{model}/'\n",
    "\n",
    "vectors_c, vectors_u, _ = load_vectors(run_folder)\n",
    "labels_to_ids_c, ids_to_labels_c, labels_to_ids_u, ids_to_labels_u = load_labels(run_folder)\n",
    "run_tsne(vectors_c, ids_to_labels_c, perplexity=30, type_labels='constituency', seed=123)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "Y3W6O23_sqlX",
    "outputId": "efc6de89-85f3-4ef1-e4a6-98a3ec243e2e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "run_folder = 'runs/multilingual_CodeBERTrand-baseline/'\n",
    "vectors_c, vectors_u, _ = load_vectors(run_folder)\n",
    "labels_to_ids_c, ids_to_labels_c, labels_to_ids_u, ids_to_labels_u = load_labels(run_folder)\n",
    "run_tsne(vectors_c, ids_to_labels_c, perplexity=30, type_labels='constituency', seed=123)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "o5xkoyYQtxLB",
    "outputId": "d700bb2c-c745-413a-942f-c85683d2673a"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
